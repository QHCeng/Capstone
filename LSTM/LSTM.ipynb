{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5881ce3e",
   "metadata": {},
   "source": [
    "User Segmentation Using RFM and debt/income/credit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14165649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer\n",
    "import calendar\n",
    "import holidays\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb53a6b",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ee3645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 13,305,915 rows and 36 columns\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path.cwd().parent / 'merged-df.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded dataset with {df.shape[0]:,} rows and {df.shape[1]:,} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac1b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1,330,592 rows (10.00% of original) for development\n"
     ]
    }
   ],
   "source": [
    "# sample 10% of the rows for dev\n",
    "_original_n = len(df)\n",
    "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Sampled {len(df):,} rows ({len(df)/_original_n:.2%} of original) for development\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7ec7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1330592 entries, 0 to 1330591\n",
      "Data columns (total 38 columns):\n",
      " #   Column                      Non-Null Count    Dtype         \n",
      "---  ------                      --------------    -----         \n",
      " 0   transaction_id              1330592 non-null  int64         \n",
      " 1   date                        1330592 non-null  datetime64[ns]\n",
      " 2   user_id                     1330592 non-null  int64         \n",
      " 3   card_id                     1330592 non-null  int64         \n",
      " 4   amount                      1330592 non-null  float64       \n",
      " 5   use_chip                    1330592 non-null  object        \n",
      " 6   merchant_id                 1330592 non-null  int64         \n",
      " 7   merchant_city               1330592 non-null  object        \n",
      " 8   merchant_state              1330592 non-null  object        \n",
      " 9   zip                         1330592 non-null  object        \n",
      " 10  mcc                         1330592 non-null  int64         \n",
      " 11  description                 1330592 non-null  object        \n",
      " 12  category                    1330592 non-null  object        \n",
      " 13  user_current_age            1330592 non-null  int64         \n",
      " 14  user_retirement_age         1330592 non-null  int64         \n",
      " 15  user_birth_year             1330592 non-null  int64         \n",
      " 16  user_birth_month            1330592 non-null  int64         \n",
      " 17  user_gender                 1330592 non-null  object        \n",
      " 18  user_address                1330592 non-null  object        \n",
      " 19  user_latitude               1330592 non-null  float64       \n",
      " 20  user_longitude              1330592 non-null  float64       \n",
      " 21  user_per_capita_income      1330592 non-null  float64       \n",
      " 22  user_yearly_income          1330592 non-null  float64       \n",
      " 23  user_total_debt             1330592 non-null  float64       \n",
      " 24  user_credit_score           1330592 non-null  int64         \n",
      " 25  user_num_credit_cards       1330592 non-null  int64         \n",
      " 26  card_card_brand             1330592 non-null  object        \n",
      " 27  card_card_type              1330592 non-null  object        \n",
      " 28  card_has_chip               1330592 non-null  object        \n",
      " 29  card_cvv                    1330592 non-null  int64         \n",
      " 30  card_expires                1330592 non-null  object        \n",
      " 31  card_num_cards_issued       1330592 non-null  int64         \n",
      " 32  card_credit_limit           1330592 non-null  float64       \n",
      " 33  card_acct_open_date         1330592 non-null  object        \n",
      " 34  card_year_pin_last_changed  1330592 non-null  int64         \n",
      " 35  card_card_on_dark_web       1330592 non-null  object        \n",
      " 36  month                       1330592 non-null  datetime64[ns]\n",
      " 37  is_weekend                  1330592 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](2), float64(7), int64(14), object(14)\n",
      "memory usage: 376.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9209d60d",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>card_card_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11978328</td>\n",
       "      <td>263.43</td>\n",
       "      <td>54850</td>\n",
       "      <td>Home &amp; Utilities</td>\n",
       "      <td>Telecommunication Services</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11363233</td>\n",
       "      <td>38.26</td>\n",
       "      <td>68135</td>\n",
       "      <td>Food &amp; Dining</td>\n",
       "      <td>Grocery Stores, Supermarkets</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8117710</td>\n",
       "      <td>52.57</td>\n",
       "      <td>81833</td>\n",
       "      <td>Shopping &amp; Retail</td>\n",
       "      <td>Drug Stores and Pharmacies</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12606562</td>\n",
       "      <td>40.00</td>\n",
       "      <td>27092</td>\n",
       "      <td>Financial &amp; Insurance</td>\n",
       "      <td>Money Transfer</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12628171</td>\n",
       "      <td>4.58</td>\n",
       "      <td>44578</td>\n",
       "      <td>Food &amp; Dining</td>\n",
       "      <td>Eating Places and Restaurants</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330587</th>\n",
       "      <td>22261028</td>\n",
       "      <td>8.19</td>\n",
       "      <td>9343</td>\n",
       "      <td>Food &amp; Dining</td>\n",
       "      <td>Eating Places and Restaurants</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330588</th>\n",
       "      <td>12782263</td>\n",
       "      <td>38.46</td>\n",
       "      <td>61195</td>\n",
       "      <td>Transportation &amp; Travel</td>\n",
       "      <td>Service Stations</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330589</th>\n",
       "      <td>15502502</td>\n",
       "      <td>44.75</td>\n",
       "      <td>20519</td>\n",
       "      <td>Food &amp; Dining</td>\n",
       "      <td>Book Stores</td>\n",
       "      <td>Credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330590</th>\n",
       "      <td>12467135</td>\n",
       "      <td>49.03</td>\n",
       "      <td>43293</td>\n",
       "      <td>Food &amp; Dining</td>\n",
       "      <td>Miscellaneous Food Stores</td>\n",
       "      <td>Credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330591</th>\n",
       "      <td>22731256</td>\n",
       "      <td>30.32</td>\n",
       "      <td>16798</td>\n",
       "      <td>Transportation &amp; Travel</td>\n",
       "      <td>Taxicabs and Limousines</td>\n",
       "      <td>Credit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1263462 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         transaction_id  amount  merchant_id                 category                    description card_card_type\n",
       "0              11978328  263.43        54850         Home & Utilities     Telecommunication Services          Debit\n",
       "1              11363233   38.26        68135            Food & Dining   Grocery Stores, Supermarkets          Debit\n",
       "2               8117710   52.57        81833        Shopping & Retail     Drug Stores and Pharmacies          Debit\n",
       "3              12606562   40.00        27092    Financial & Insurance                 Money Transfer          Debit\n",
       "4              12628171    4.58        44578            Food & Dining  Eating Places and Restaurants          Debit\n",
       "...                 ...     ...          ...                      ...                            ...            ...\n",
       "1330587        22261028    8.19         9343            Food & Dining  Eating Places and Restaurants          Debit\n",
       "1330588        12782263   38.46        61195  Transportation & Travel               Service Stations          Debit\n",
       "1330589        15502502   44.75        20519            Food & Dining                    Book Stores         Credit\n",
       "1330590        12467135   49.03        43293            Food & Dining      Miscellaneous Food Stores         Credit\n",
       "1330591        22731256   30.32        16798  Transportation & Travel        Taxicabs and Limousines         Credit\n",
       "\n",
       "[1263462 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neg_tx = df[df['amount'] > 0].copy()\n",
    "display(neg_tx[[\"transaction_id\", \"amount\", \"merchant_id\", \"category\", \"description\", \"card_card_type\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fad022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13305915 entries, 0 to 13305914\n",
      "Data columns (total 36 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   transaction_id              int64  \n",
      " 1   date                        object \n",
      " 2   user_id                     int64  \n",
      " 3   card_id                     int64  \n",
      " 4   amount                      float64\n",
      " 5   use_chip                    object \n",
      " 6   merchant_id                 int64  \n",
      " 7   merchant_city               object \n",
      " 8   merchant_state              object \n",
      " 9   zip                         object \n",
      " 10  mcc                         int64  \n",
      " 11  description                 object \n",
      " 12  category                    object \n",
      " 13  user_current_age            int64  \n",
      " 14  user_retirement_age         int64  \n",
      " 15  user_birth_year             int64  \n",
      " 16  user_birth_month            int64  \n",
      " 17  user_gender                 object \n",
      " 18  user_address                object \n",
      " 19  user_latitude               float64\n",
      " 20  user_longitude              float64\n",
      " 21  user_per_capita_income      float64\n",
      " 22  user_yearly_income          float64\n",
      " 23  user_total_debt             float64\n",
      " 24  user_credit_score           int64  \n",
      " 25  user_num_credit_cards       int64  \n",
      " 26  card_card_brand             object \n",
      " 27  card_card_type              object \n",
      " 28  card_has_chip               object \n",
      " 29  card_cvv                    int64  \n",
      " 30  card_expires                object \n",
      " 31  card_num_cards_issued       int64  \n",
      " 32  card_credit_limit           float64\n",
      " 33  card_acct_open_date         object \n",
      " 34  card_year_pin_last_changed  int64  \n",
      " 35  card_card_on_dark_web       object \n",
      "dtypes: float64(7), int64(14), object(15)\n",
      "memory usage: 3.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d1a57",
   "metadata": {},
   "source": [
    "# Problem definition — monthly per-customer forecasting\n",
    "\n",
    "Goal\n",
    "- Granularity: monthly per customer.\n",
    "- Series: for each user_id define $amt_t$ = monthly total spend for month t.\n",
    "- Look-back (LB): 12 months (t-11 … t).\n",
    "- Target: $y_{t+1} = amt_{t+1}$ (next month).\n",
    "- Supervised windows: for each user and cut-off t build one training row containing:\n",
    "  - past-known covariates (lags, rolling windows, expanding stats computed using ≤ t)\n",
    "  - future-known covariates (calendar/holiday dummies for t+1)\n",
    "  - label: $amt_{t+1}$\n",
    "- Data-leakage rule: Every feature for a window ending at month t must use only data ≤ t. Practically: always shift the series (e.g., `.shift(1)` or `.shift(lag)`) before computing rolling/expanding aggregates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43c4ed",
   "metadata": {},
   "source": [
    "# Core transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 1,219  Months: 118  est rows (users x months): 143,842\n",
      "Performing global user x months reindex (fills every user for every month in the global span).\n",
      "monthly shape after reindex: (143842, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>amt_t</th>\n",
       "      <th>txn_cnt_t</th>\n",
       "      <th>avg_txn_amt_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>547.05</td>\n",
       "      <td>10</td>\n",
       "      <td>54.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>864.79</td>\n",
       "      <td>11</td>\n",
       "      <td>78.617273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>783.05</td>\n",
       "      <td>14</td>\n",
       "      <td>55.932143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>857.40</td>\n",
       "      <td>8</td>\n",
       "      <td>107.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>112.22</td>\n",
       "      <td>5</td>\n",
       "      <td>22.444000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      month   amt_t  txn_cnt_t  avg_txn_amt_t\n",
       "0        0 2010-01-01  547.05         10      54.705000\n",
       "1        0 2010-02-01  864.79         11      78.617273\n",
       "2        0 2010-03-01  783.05         14      55.932143\n",
       "3        0 2010-04-01  857.40          8     107.175000\n",
       "4        0 2010-05-01  112.22          5      22.444000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_start: 2017-11-01 00:00:00 test_start: 2018-11-01 00:00:00\n",
      "Feature columns available for scaling: ['amt_t_trans', 'txn_cnt_t', 'avg_txn_amt_t_wins']\n",
      "\n",
      "Core transforms complete.\n",
      "monthly (post-core) shape: (143842, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>amt_t</th>\n",
       "      <th>txn_cnt_t</th>\n",
       "      <th>avg_txn_amt_t</th>\n",
       "      <th>amt_q01</th>\n",
       "      <th>amt_q99</th>\n",
       "      <th>amt_t_wins</th>\n",
       "      <th>avg_q01</th>\n",
       "      <th>avg_q99</th>\n",
       "      <th>avg_txn_amt_t_wins</th>\n",
       "      <th>amt_t_trans</th>\n",
       "      <th>amt_t_trans_s</th>\n",
       "      <th>txn_cnt_t_s</th>\n",
       "      <th>avg_txn_amt_t_wins_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>547.05</td>\n",
       "      <td>10</td>\n",
       "      <td>54.705000</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>1638.0521</td>\n",
       "      <td>547.05</td>\n",
       "      <td>-23.590975</td>\n",
       "      <td>124.533069</td>\n",
       "      <td>54.705000</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>0.521694</td>\n",
       "      <td>0.141389</td>\n",
       "      <td>0.371876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>864.79</td>\n",
       "      <td>11</td>\n",
       "      <td>78.617273</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>1638.0521</td>\n",
       "      <td>864.79</td>\n",
       "      <td>-23.590975</td>\n",
       "      <td>124.533069</td>\n",
       "      <td>78.617273</td>\n",
       "      <td>6.763642</td>\n",
       "      <td>0.726850</td>\n",
       "      <td>0.313203</td>\n",
       "      <td>1.065374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>783.05</td>\n",
       "      <td>14</td>\n",
       "      <td>55.932143</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>1638.0521</td>\n",
       "      <td>783.05</td>\n",
       "      <td>-23.590975</td>\n",
       "      <td>124.533069</td>\n",
       "      <td>55.932143</td>\n",
       "      <td>6.664473</td>\n",
       "      <td>0.682358</td>\n",
       "      <td>0.828644</td>\n",
       "      <td>0.407466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>857.40</td>\n",
       "      <td>8</td>\n",
       "      <td>107.175000</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>1638.0521</td>\n",
       "      <td>857.40</td>\n",
       "      <td>-23.590975</td>\n",
       "      <td>124.533069</td>\n",
       "      <td>107.175000</td>\n",
       "      <td>6.755070</td>\n",
       "      <td>0.723004</td>\n",
       "      <td>-0.202239</td>\n",
       "      <td>1.893597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>112.22</td>\n",
       "      <td>5</td>\n",
       "      <td>22.444000</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>1638.0521</td>\n",
       "      <td>112.22</td>\n",
       "      <td>-23.590975</td>\n",
       "      <td>124.533069</td>\n",
       "      <td>22.444000</td>\n",
       "      <td>4.729333</td>\n",
       "      <td>-0.185838</td>\n",
       "      <td>-0.717680</td>\n",
       "      <td>-0.563748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      month   amt_t  txn_cnt_t  avg_txn_amt_t   amt_q01    amt_q99  \\\n",
       "0        0 2010-01-01  547.05         10      54.705000 -211.9624  1638.0521   \n",
       "1        0 2010-02-01  864.79         11      78.617273 -211.9624  1638.0521   \n",
       "2        0 2010-03-01  783.05         14      55.932143 -211.9624  1638.0521   \n",
       "3        0 2010-04-01  857.40          8     107.175000 -211.9624  1638.0521   \n",
       "4        0 2010-05-01  112.22          5      22.444000 -211.9624  1638.0521   \n",
       "\n",
       "   amt_t_wins    avg_q01     avg_q99  avg_txn_amt_t_wins  amt_t_trans  \\\n",
       "0      547.05 -23.590975  124.533069           54.705000     6.306367   \n",
       "1      864.79 -23.590975  124.533069           78.617273     6.763642   \n",
       "2      783.05 -23.590975  124.533069           55.932143     6.664473   \n",
       "3      857.40 -23.590975  124.533069          107.175000     6.755070   \n",
       "4      112.22 -23.590975  124.533069           22.444000     4.729333   \n",
       "\n",
       "   amt_t_trans_s  txn_cnt_t_s  avg_txn_amt_t_wins_s  \n",
       "0       0.521694     0.141389              0.371876  \n",
       "1       0.726850     0.313203              1.065374  \n",
       "2       0.682358     0.828644              0.407466  \n",
       "3       0.723004    -0.202239              1.893597  \n",
       "4      -0.185838    -0.717680             -0.563748  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters (tweakable)\n",
    "Q_LOW = 0.01\n",
    "Q_HIGH = 0.99\n",
    "REINDEX_GLOBAL_THRESHOLD = 50_000_000  # heuristic: if users * months <= this we will expand globally\n",
    "REINDEX_GLOBAL = False                 # set True to force global user x month grid (only if you understand memory)\n",
    "DO_YEO_JOHNSON = False                 # if True, use PowerTransformer(method='yeo-johnson') fitted on train for amt\n",
    "DO_SIGNED_LOG = True                   # if True (and YE0 not used) use sign(x)*log1p(|x|)\n",
    "SCALER_TYPE = 'standard'               # options: 'standard', 'robust', 'minmax'\n",
    "SCALED_FEAT_SUFFIX = '_s'\n",
    "\n",
    "# Safety checks\n",
    "required_cols = {'date', 'user_id', 'amount'}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns in df: {missing}\")\n",
    "\n",
    "# 1) monthly aggregation (amt_t, txn_cnt_t, avg_txn_amt_t)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['month'] = df['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "count_col = 'transaction_id' if 'transaction_id' in df.columns else None\n",
    "if count_col:\n",
    "    monthly = (\n",
    "        df\n",
    "        .groupby(['user_id', 'month'], as_index=False)\n",
    "        .agg(amt_t=('amount', 'sum'), txn_cnt_t=(count_col, 'count'))\n",
    "    )\n",
    "else:\n",
    "    monthly = (\n",
    "        df\n",
    "        .groupby(['user_id', 'month'], as_index=False)\n",
    "        .agg(amt_t=('amount', 'sum'))\n",
    "    )\n",
    "    monthly['txn_cnt_t'] = (\n",
    "        df\n",
    "        .groupby(['user_id', 'month'])\n",
    "        .size()\n",
    "        .values\n",
    "    )\n",
    "\n",
    "monthly['avg_txn_amt_t'] = monthly['amt_t'] / monthly['txn_cnt_t'].clip(lower=1)\n",
    "\n",
    "# 2) reindex to fill missing months per user (choose global or per-user min/max)\n",
    "all_months = pd.date_range(monthly['month'].min(), monthly['month'].max(), freq='MS')\n",
    "users = monthly['user_id'].unique()\n",
    "estimated_rows = len(users) * len(all_months)\n",
    "print(f\"Users: {len(users):,}  Months: {len(all_months):,}  est rows (users x months): {estimated_rows:,}\")\n",
    "\n",
    "if REINDEX_GLOBAL or estimated_rows <= REINDEX_GLOBAL_THRESHOLD:\n",
    "    # global full reindex (user x full date range)\n",
    "    print(\"Performing global user x months reindex (fills every user for every month in the global span).\")\n",
    "    full_index = pd.MultiIndex.from_product([users, all_months], names=['user_id', 'month'])\n",
    "    monthly = (\n",
    "        monthly\n",
    "        .set_index(['user_id', 'month'])\n",
    "        .reindex(full_index, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    # memory-saving per-user reindex: fill each user's missing months between their min and max month\n",
    "    print(\"Large expansion avoided — reindexing each user to their local min/max months (memory-saving). This still fills missing months with zeros.\")\n",
    "    parts = []\n",
    "    for uid, g in monthly.groupby('user_id'):\n",
    "        idx = pd.date_range(g['month'].min(), g['month'].max(), freq='MS')\n",
    "        g2 = (\n",
    "            g\n",
    "            .set_index('month')\n",
    "            .reindex(idx, fill_value=0)\n",
    "            .rename_axis('month')\n",
    "            .reset_index()\n",
    "        )\n",
    "        g2['user_id'] = uid\n",
    "        parts.append(g2)\n",
    "    monthly = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# ensure consistent dtypes\n",
    "monthly = monthly.sort_values(['user_id', 'month']).reset_index(drop=True)\n",
    "monthly['txn_cnt_t'] = monthly['txn_cnt_t'].fillna(0).astype(int)\n",
    "monthly['amt_t'] = monthly['amt_t'].fillna(0).astype(float)\n",
    "monthly['avg_txn_amt_t'] = monthly['avg_txn_amt_t'].fillna(0).astype(float)\n",
    "\n",
    "print('monthly shape after reindex:', monthly.shape)\n",
    "display(monthly.head())\n",
    "\n",
    "# 3) Define time splits used to compute train-only transforms\n",
    "max_month = monthly['month'].max()\n",
    "test_start = max_month - pd.DateOffset(months=11)   # last 12 months -> test\n",
    "val_start = test_start - pd.DateOffset(months=12)   # previous 12 months -> val\n",
    "print('val_start:', val_start, 'test_start:', test_start)\n",
    "\n",
    "train_mask = monthly['month'] < val_start\n",
    "val_mask = (monthly['month'] >= val_start) & (monthly['month'] < test_start)\n",
    "test_mask = monthly['month'] >= test_start\n",
    "\n",
    "# 4) Outlier handling (winsorize) — compute thresholds on TRAIN only and apply to all splits\n",
    "q_low, q_high = Q_LOW, Q_HIGH\n",
    "train_monthly = monthly[train_mask]\n",
    "if train_monthly.empty:\n",
    "    warnings.warn('Training partition is empty — cannot compute train-only winsorization thresholds. Using global quantiles on entire data instead.')\n",
    "    train_monthly = monthly\n",
    "\n",
    "# per-user quantiles (train-only). Users without train rows will get filled with global quantiles\n",
    "global_amt_q_low = train_monthly['amt_t'].quantile(q_low)\n",
    "global_amt_q_high = train_monthly['amt_t'].quantile(q_high)\n",
    "user_amt_q = (\n",
    "    train_monthly\n",
    "    .groupby('user_id')['amt_t']\n",
    "    .agg(amt_q01=lambda x: x.quantile(q_low), amt_q99=lambda x: x.quantile(q_high))\n",
    ")\n",
    "user_amt_q['amt_q01'] = user_amt_q['amt_q01'].fillna(global_amt_q_low)\n",
    "user_amt_q['amt_q99'] = user_amt_q['amt_q99'].fillna(global_amt_q_high)\n",
    "\n",
    "monthly = monthly.merge(user_amt_q[['amt_q01', 'amt_q99']], left_on='user_id', right_index=True, how='left')\n",
    "monthly['amt_q01'] = monthly['amt_q01'].fillna(global_amt_q_low)\n",
    "monthly['amt_q99'] = monthly['amt_q99'].fillna(global_amt_q_high)\n",
    "monthly['amt_t_wins'] = monthly['amt_t'].clip(lower=monthly['amt_q01'], upper=monthly['amt_q99'])\n",
    "\n",
    "# avg_txn_amt winsorization (same approach)\n",
    "global_avg_q_low = train_monthly['avg_txn_amt_t'].quantile(q_low)\n",
    "global_avg_q_high = train_monthly['avg_txn_amt_t'].quantile(q_high)\n",
    "user_avg_q = (\n",
    "    train_monthly\n",
    "    .groupby('user_id')['avg_txn_amt_t']\n",
    "    .agg(avg_q01=lambda x: x.quantile(q_low), avg_q99=lambda x: x.quantile(q_high))\n",
    ")\n",
    "user_avg_q['avg_q01'] = user_avg_q['avg_q01'].fillna(global_avg_q_low)\n",
    "user_avg_q['avg_q99'] = user_avg_q['avg_q99'].fillna(global_avg_q_high)\n",
    "\n",
    "monthly = monthly.merge(user_avg_q[['avg_q01', 'avg_q99']], left_on='user_id', right_index=True, how='left')\n",
    "monthly['avg_q01'] = monthly['avg_q01'].fillna(global_avg_q_low)\n",
    "monthly['avg_q99'] = monthly['avg_q99'].fillna(global_avg_q_high)\n",
    "monthly['avg_txn_amt_t_wins'] = monthly['avg_txn_amt_t'].clip(lower=monthly['avg_q01'], upper=monthly['avg_q99'])\n",
    "\n",
    "# 5) Variance stabilization (optional). Default: sign-preserving log1p. Alternate: Yeo-Johnson fitted on train only.\n",
    "if DO_YEO_JOHNSON:\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    # fit on the winsorized training values\n",
    "    pt.fit(monthly.loc[train_mask, ['amt_t_wins']].values.reshape(-1, 1))\n",
    "    monthly['amt_t_trans'] = pt.transform(monthly[['amt_t_wins']])\n",
    "    core_transformers = {'amt_yeo': pt}\n",
    "else:\n",
    "    # signed log1p: preserves sign and compresses magnitude, safe for negatives/zeros\n",
    "    monthly['amt_t_trans'] = np.sign(monthly['amt_t_wins']) * np.log1p(np.abs(monthly['amt_t_wins']))\n",
    "    core_transformers = {}\n",
    "\n",
    "# 6) Scaling: fit scaler on TRAIN features only and apply to all data. Keep scaler object for inverse transform.\n",
    "feature_candidates = ['amt_t_trans', 'txn_cnt_t', 'avg_txn_amt_t_wins']\n",
    "feature_cols = [c for c in feature_candidates if c in monthly.columns]\n",
    "print('Feature columns available for scaling:', feature_cols)\n",
    "\n",
    "scaler_cls = {'standard': StandardScaler, 'robust': RobustScaler, 'minmax': MinMaxScaler}[SCALER_TYPE]\n",
    "feature_scaler = scaler_cls()\n",
    "feature_scaler.fit(monthly.loc[train_mask, feature_cols].fillna(0))\n",
    "scaled_cols = [c + SCALED_FEAT_SUFFIX for c in feature_cols]\n",
    "monthly[scaled_cols] = feature_scaler.transform(monthly[feature_cols].fillna(0))\n",
    "\n",
    "# Save artifacts for downstream use (winsorization thresholds, scaler, etc.)\n",
    "core_transforms = {\n",
    "    'q_low': q_low,\n",
    "    'q_high': q_high,\n",
    "    'global_amt_q': (global_amt_q_low, global_amt_q_high),\n",
    "    'global_avg_q': (global_avg_q_low, global_avg_q_high),\n",
    "    'user_amt_q': user_amt_q,           # DataFrame keyed by user_id with amt_q01/amt_q99\n",
    "    'user_avg_q': user_avg_q,           # DataFrame keyed by user_id with avg_q01/avg_q99\n",
    "    'feature_scaler': feature_scaler,\n",
    "    'scaler_features': feature_cols,\n",
    "    'scaler_suffix': SCALED_FEAT_SUFFIX,\n",
    "    'transformers': core_transformers,\n",
    "    'reindex_strategy': 'global' if (REINDEX_GLOBAL or estimated_rows <= REINDEX_GLOBAL_THRESHOLD) else 'per_user_minmax',\n",
    "    'val_start': val_start,\n",
    "    'test_start': test_start\n",
    "}\n",
    "\n",
    "print('\\nCore transforms complete.')\n",
    "print('monthly (post-core) shape:', monthly.shape)\n",
    "display(monthly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde96919",
   "metadata": {},
   "source": [
    "# Temporal & seasonal features\n",
    "\n",
    "These features are known at forecast time and safe to include as future-known covariates. This cell will add:\n",
    "\n",
    "- calendar one-hots (month_of_year, quarter),\n",
    "- Fourier seasonality features (monthly, order K) for both t and t+1,\n",
    "- holiday / event presence flags (country-specific via python-holidays when available) and a Black Friday detector,\n",
    "- lead/lag event windows (month-level flags) such as \"is_month_before_black_friday\" and \"is_week_before_black_friday\",\n",
    "- per-user Seasonal Index SI_{u,m} computed on TRAIN only: median(amt) for user & month / median(amt) overall for that user.\n",
    "\n",
    "All user-level statistics (seasonal index) are computed using the training partition only to avoid leakage. The resulting features are attached for both t and t+1 so they can be joined into supervised windows later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d01fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal & seasonal features added. Columns now: ['mo_1', 'mo_10', 'mo_11', 'mo_12', 'mo_2', 'mo_3', 'mo_4', 'mo_5', 'mo_6', 'mo_7', 'mo_8', 'mo_9', 'mo_1', 'mo_10', 'mo_11', 'mo_12', 'mo_2', 'mo_3', 'mo_4', 'mo_5', 'mo_6', 'mo_7', 'mo_8', 'mo_9', 'sin_1', 'sin_2', 'sin_3', 'sin_4', 'is_holiday', 'is_black_friday', 'is_month_before_black_friday', 'seasonal_index', 'seasonal_index_tplus1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>amt_t_wins</th>\n",
       "      <th>seasonal_index</th>\n",
       "      <th>seasonal_index_tplus1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>1.050831</td>\n",
       "      <td>0.683537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>0.683537</td>\n",
       "      <td>1.362648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>1.362648</td>\n",
       "      <td>1.040518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>1.040518</td>\n",
       "      <td>0.873485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>0.873485</td>\n",
       "      <td>1.395791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>1.395791</td>\n",
       "      <td>1.356828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>1.356828</td>\n",
       "      <td>1.166422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>1.166422</td>\n",
       "      <td>0.633069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>0.633069</td>\n",
       "      <td>0.827778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>1.187411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      month  amt_t_wins  seasonal_index  seasonal_index_tplus1\n",
       "0        0 2010-01-01    547.0500        1.050831               0.683537\n",
       "1        0 2010-02-01    864.7900        0.683537               1.362648\n",
       "2        0 2010-03-01    783.0500        1.362648               1.040518\n",
       "3        0 2010-04-01    857.4000        1.040518               0.873485\n",
       "4        0 2010-05-01    112.2200        0.873485               1.395791\n",
       "5        0 2010-06-01    268.5300        1.395791               1.356828\n",
       "6        0 2010-07-01    719.0300        1.356828               1.166422\n",
       "7        0 2010-08-01    535.2000        1.166422               0.633069\n",
       "8        0 2010-09-01   -211.9624        0.633069               0.827778\n",
       "9        0 2010-10-01    262.5600        0.827778               1.187411"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for seasonal features\n",
    "FOURIER_ORDER = 4  # K: number of harmonics to include\n",
    "COUNTRY = 'US'     # country code for holidays (holidays import assumed at top)\n",
    "\n",
    "m = 12  # monthly period\n",
    "\n",
    "# base calendar features on the month timestamp (start of month)\n",
    "monthly['month_of_year'] = monthly['month'].dt.month\n",
    "monthly['quarter'] = monthly['month'].dt.quarter\n",
    "\n",
    "# one-hot months and quarter (sparse if many users/months)\n",
    "month_dummies = pd.get_dummies(monthly['month_of_year'].astype(int).astype(str), prefix='mo')\n",
    "quarter_dummies = pd.get_dummies(monthly['quarter'].astype(int).astype(str), prefix='q')\n",
    "monthly = pd.concat([monthly, month_dummies, quarter_dummies], axis=1)\n",
    "\n",
    "# Fourier terms (for month index)\n",
    "monthly = monthly.reset_index(drop=True)\n",
    "# Create an integer month index (months since the global minimum month) — robust and avoids using unsupported np.timedelta64('M')\n",
    "base = monthly['month'].min()\n",
    "base_idx = base.year * 12 + base.month\n",
    "monthly['month_index'] = monthly['month'].dt.year * 12 + monthly['month'].dt.month - base_idx\n",
    "for k in range(1, FOURIER_ORDER + 1):\n",
    "    monthly[f'sin_{k}'] = np.sin(2 * np.pi * k * monthly['month_index'] / m)\n",
    "    monthly[f'cos_{k}'] = np.cos(2 * np.pi * k * monthly['month_index'] / m)\n",
    "\n",
    "# Holidays and events (assumes holidays package available at top-level)\n",
    "monthly['is_holiday'] = False\n",
    "monthly['is_black_friday'] = False\n",
    "\n",
    "yrs = range(monthly['month'].dt.year.min(), monthly['month'].dt.year.max() + 1)\n",
    "hols = holidays.CountryHoliday(COUNTRY, years=yrs)\n",
    "# For monthly data, mark a month as holiday-month if it contains any holiday\n",
    "holiday_months = set(pd.to_datetime(list(hols.keys())).to_period('M').to_timestamp())\n",
    "monthly['is_holiday'] = monthly['month'].isin(holiday_months)\n",
    "\n",
    "# Black Friday detection: assume Black Friday = day after US Thanksgiving (4th Thu in Nov)\n",
    "# Mark the month of Black Friday and a month-before flag to capture pre-event effects\n",
    "\n",
    "def black_friday_months(year):\n",
    "    # Thanksgiving: fourth Thursday in November\n",
    "    nov = calendar.monthcalendar(year, 11)\n",
    "    # find the Thursday index for the 4th Thursday\n",
    "    thursdays = [week[calendar.THURSDAY] for week in nov if week[calendar.THURSDAY] != 0]\n",
    "    if len(thursdays) >= 4:\n",
    "        thanksgiving_day = thursdays[3]\n",
    "    else:\n",
    "        thanksgiving_day = thursdays[-1]\n",
    "    bf = pd.Timestamp(year=year, month=11, day=thanksgiving_day + 1)\n",
    "    return bf.to_period('M').to_timestamp()\n",
    "\n",
    "bf_months = {black_friday_months(y) for y in range(monthly['month'].dt.year.min(), monthly['month'].dt.year.max() + 1)}\n",
    "monthly['is_black_friday'] = monthly['month'].isin(bf_months)\n",
    "monthly['is_month_before_black_friday'] = monthly['month'].isin([m_ - pd.DateOffset(months=1) for m_ in bf_months])\n",
    "\n",
    "# -----------------------------\n",
    "# Per-user Seasonal Index (SI_{u,m}) — VECTORISED (TRAIN ONLY)\n",
    "# -----------------------------\n",
    "train_monthly = monthly[train_mask]\n",
    "\n",
    "# median per user overall (train only)\n",
    "user_median_all = train_monthly.groupby('user_id')['amt_t_wins'].median().rename('user_med_all')\n",
    "# median per user per month-of-year (train only)\n",
    "user_median_by_month = (\n",
    "    train_monthly\n",
    "    .groupby(['user_id', 'month_of_year'])['amt_t_wins']\n",
    "    .median()\n",
    "    .rename('user_med_m')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# merge seasonal index inputs into monthly (vectorized join — avoids apply)\n",
    "monthly = monthly.merge(user_median_all.reset_index(), on='user_id', how='left')\n",
    "monthly = monthly.merge(user_median_by_month, on=['user_id', 'month_of_year'], how='left')\n",
    "\n",
    "# guard against missing medians and zero denominators\n",
    "monthly['user_med_all'] = monthly['user_med_all'].fillna(0)\n",
    "monthly['seasonal_index'] = (monthly['user_med_m'] / monthly['user_med_all']).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "\n",
    "# attach SI for t+1 (month t+1) using a vectorized join on (user_id, month_tplus1_mo)\n",
    "monthly['month_tplus1'] = monthly['month'] + pd.DateOffset(months=1)\n",
    "monthly['month_tplus1_mo'] = monthly['month_tplus1'].dt.month\n",
    "user_month_map_t1 = user_median_by_month.rename(columns={'month_of_year': 'month_tplus1_mo', 'user_med_m': 'user_med_m_tplus1'})\n",
    "monthly = monthly.merge(user_month_map_t1[['user_id', 'month_tplus1_mo', 'user_med_m_tplus1']], on=['user_id', 'month_tplus1_mo'], how='left')\n",
    "monthly['seasonal_index_tplus1'] = (monthly['user_med_m_tplus1'] / monthly['user_med_all']).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "\n",
    "# season tag one-hot\n",
    "monthly['season'] = ((monthly['month_of_year'] % 12 + 3) // 3).map({1: 'winter', 2: 'spring', 3: 'summer', 4: 'fall'})\n",
    "season_dummies = pd.get_dummies(monthly['season'], prefix='sea')\n",
    "monthly = pd.concat([monthly, season_dummies], axis=1)\n",
    "\n",
    "print('Temporal & seasonal features added. Columns now:', [c for c in monthly.columns if 'seasonal' in c or c.startswith('mo_') or c.startswith('sin_') or c.startswith('is_')][:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910248f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>amt_t_wins</th>\n",
       "      <th>seasonal_index</th>\n",
       "      <th>seasonal_index_tplus1</th>\n",
       "      <th>is_black_friday</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>1.050831</td>\n",
       "      <td>0.683537</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>0.683537</td>\n",
       "      <td>1.362648</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>1.362648</td>\n",
       "      <td>1.040518</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>1.040518</td>\n",
       "      <td>0.873485</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>0.873485</td>\n",
       "      <td>1.395791</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>1.395791</td>\n",
       "      <td>1.356828</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>1.356828</td>\n",
       "      <td>1.166422</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>1.166422</td>\n",
       "      <td>0.633069</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>0.633069</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>1.187411</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>250.6300</td>\n",
       "      <td>1.187411</td>\n",
       "      <td>0.795818</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>153.6400</td>\n",
       "      <td>0.795818</td>\n",
       "      <td>1.050831</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1062.8000</td>\n",
       "      <td>1.050831</td>\n",
       "      <td>0.683537</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>-210.7500</td>\n",
       "      <td>0.683537</td>\n",
       "      <td>1.362648</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>833.6100</td>\n",
       "      <td>1.362648</td>\n",
       "      <td>1.040518</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id      month  amt_t_wins  seasonal_index  seasonal_index_tplus1  \\\n",
       "0         0 2010-01-01    547.0500        1.050831               0.683537   \n",
       "1         0 2010-02-01    864.7900        0.683537               1.362648   \n",
       "2         0 2010-03-01    783.0500        1.362648               1.040518   \n",
       "3         0 2010-04-01    857.4000        1.040518               0.873485   \n",
       "4         0 2010-05-01    112.2200        0.873485               1.395791   \n",
       "5         0 2010-06-01    268.5300        1.395791               1.356828   \n",
       "6         0 2010-07-01    719.0300        1.356828               1.166422   \n",
       "7         0 2010-08-01    535.2000        1.166422               0.633069   \n",
       "8         0 2010-09-01   -211.9624        0.633069               0.827778   \n",
       "9         0 2010-10-01    262.5600        0.827778               1.187411   \n",
       "10        0 2010-11-01    250.6300        1.187411               0.795818   \n",
       "11        0 2010-12-01    153.6400        0.795818               1.050831   \n",
       "12        0 2011-01-01   1062.8000        1.050831               0.683537   \n",
       "13        0 2011-02-01   -210.7500        0.683537               1.362648   \n",
       "14        0 2011-03-01    833.6100        1.362648               1.040518   \n",
       "\n",
       "    is_black_friday  is_holiday  \n",
       "0             False        True  \n",
       "1             False        True  \n",
       "2             False       False  \n",
       "3             False       False  \n",
       "4             False        True  \n",
       "5             False       False  \n",
       "6             False        True  \n",
       "7             False       False  \n",
       "8             False        True  \n",
       "9             False        True  \n",
       "10             True        True  \n",
       "11            False        True  \n",
       "12            False        True  \n",
       "13            False        True  \n",
       "14            False       False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly[['user_id','month','amt_t_wins','seasonal_index','seasonal_index_tplus1', 'is_black_friday', 'is_holiday']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09846a2",
   "metadata": {},
   "source": [
    "# Lag features (amount & count lags, seasonal lags, seasonal diffs)\n",
    "\n",
    "This cell computes classic lag features per user using groupby.shift to avoid leakage. By default it will create compact lag set [1,3,6,12] plus seasonal lags (12, 24) and seasonal difference s_diff_12 computed on the winsorized amount (and optionally on log-transformed values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb5740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created lag columns; sample columns: ['lag_amt_1', 'lag_amt_trans_1', 'lag_amt_3', 'lag_amt_trans_3', 'lag_amt_6', 'lag_amt_trans_6', 'lag_amt_12', 'lag_amt_trans_12', 'lag_amt_24', 'lag_amt_trans_24', 'lag_cnt_1', 'lag_cnt_3', 'lag_cnt_6', 'lag_cnt_12']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>lag_amt_1</th>\n",
       "      <th>lag_amt_trans_1</th>\n",
       "      <th>lag_cnt_1</th>\n",
       "      <th>lag_amt_3</th>\n",
       "      <th>lag_amt_trans_3</th>\n",
       "      <th>lag_cnt_3</th>\n",
       "      <th>lag_amt_6</th>\n",
       "      <th>lag_amt_trans_6</th>\n",
       "      <th>lag_cnt_6</th>\n",
       "      <th>lag_amt_12</th>\n",
       "      <th>lag_amt_trans_12</th>\n",
       "      <th>lag_cnt_12</th>\n",
       "      <th>lag_amt_12</th>\n",
       "      <th>lag_amt_trans_12</th>\n",
       "      <th>s_diff_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>6.763642</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>6.664473</td>\n",
       "      <td>14.0</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>6.755070</td>\n",
       "      <td>8.0</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>6.763642</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>4.729333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>6.664473</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>5.596680</td>\n",
       "      <td>9.0</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>6.755070</td>\n",
       "      <td>8.0</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>6.579293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>4.729333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>6.763642</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>6.284507</td>\n",
       "      <td>10.0</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>5.596680</td>\n",
       "      <td>9.0</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>6.664473</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>-5.361116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>6.579293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>6.755070</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>5.574281</td>\n",
       "      <td>10.0</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>6.284507</td>\n",
       "      <td>10.0</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>4.729333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>250.6300</td>\n",
       "      <td>5.527960</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>-5.361116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>5.596680</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>153.6400</td>\n",
       "      <td>5.041100</td>\n",
       "      <td>9.0</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>5.574281</td>\n",
       "      <td>10.0</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>6.579293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>547.05</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>10.0</td>\n",
       "      <td>547.05</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>515.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>1062.8000</td>\n",
       "      <td>6.969603</td>\n",
       "      <td>9.0</td>\n",
       "      <td>250.6300</td>\n",
       "      <td>5.527960</td>\n",
       "      <td>10.0</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>6.284507</td>\n",
       "      <td>10.0</td>\n",
       "      <td>864.79</td>\n",
       "      <td>6.763642</td>\n",
       "      <td>11.0</td>\n",
       "      <td>864.79</td>\n",
       "      <td>6.763642</td>\n",
       "      <td>-1075.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>-210.7500</td>\n",
       "      <td>-5.355406</td>\n",
       "      <td>10.0</td>\n",
       "      <td>153.6400</td>\n",
       "      <td>5.041100</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>-5.361116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>783.05</td>\n",
       "      <td>6.664473</td>\n",
       "      <td>14.0</td>\n",
       "      <td>783.05</td>\n",
       "      <td>6.664473</td>\n",
       "      <td>50.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>833.6100</td>\n",
       "      <td>6.726965</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1062.8000</td>\n",
       "      <td>6.969603</td>\n",
       "      <td>9.0</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>5.574281</td>\n",
       "      <td>10.0</td>\n",
       "      <td>857.40</td>\n",
       "      <td>6.755070</td>\n",
       "      <td>8.0</td>\n",
       "      <td>857.40</td>\n",
       "      <td>6.755070</td>\n",
       "      <td>-364.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>493.2600</td>\n",
       "      <td>6.203062</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-210.7500</td>\n",
       "      <td>-5.355406</td>\n",
       "      <td>10.0</td>\n",
       "      <td>250.6300</td>\n",
       "      <td>5.527960</td>\n",
       "      <td>10.0</td>\n",
       "      <td>112.22</td>\n",
       "      <td>4.729333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>112.22</td>\n",
       "      <td>4.729333</td>\n",
       "      <td>257.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-06-01</td>\n",
       "      <td>369.5800</td>\n",
       "      <td>5.915069</td>\n",
       "      <td>9.0</td>\n",
       "      <td>833.6100</td>\n",
       "      <td>6.726965</td>\n",
       "      <td>11.0</td>\n",
       "      <td>153.6400</td>\n",
       "      <td>5.041100</td>\n",
       "      <td>9.0</td>\n",
       "      <td>268.53</td>\n",
       "      <td>5.596680</td>\n",
       "      <td>9.0</td>\n",
       "      <td>268.53</td>\n",
       "      <td>5.596680</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>284.4900</td>\n",
       "      <td>5.654207</td>\n",
       "      <td>16.0</td>\n",
       "      <td>493.2600</td>\n",
       "      <td>6.203062</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1062.8000</td>\n",
       "      <td>6.969603</td>\n",
       "      <td>9.0</td>\n",
       "      <td>719.03</td>\n",
       "      <td>6.579293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>719.03</td>\n",
       "      <td>6.579293</td>\n",
       "      <td>-294.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-08-01</td>\n",
       "      <td>424.4400</td>\n",
       "      <td>6.053124</td>\n",
       "      <td>9.0</td>\n",
       "      <td>369.5800</td>\n",
       "      <td>5.915069</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-210.7500</td>\n",
       "      <td>-5.355406</td>\n",
       "      <td>10.0</td>\n",
       "      <td>535.20</td>\n",
       "      <td>6.284507</td>\n",
       "      <td>10.0</td>\n",
       "      <td>535.20</td>\n",
       "      <td>6.284507</td>\n",
       "      <td>436.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id      month  lag_amt_1  lag_amt_trans_1  lag_cnt_1  lag_amt_3  \\\n",
       "0         0 2010-01-01        NaN              NaN        NaN        NaN   \n",
       "1         0 2010-02-01   547.0500         6.306367       10.0        NaN   \n",
       "2         0 2010-03-01   864.7900         6.763642       11.0        NaN   \n",
       "3         0 2010-04-01   783.0500         6.664473       14.0   547.0500   \n",
       "4         0 2010-05-01   857.4000         6.755070        8.0   864.7900   \n",
       "5         0 2010-06-01   112.2200         4.729333        5.0   783.0500   \n",
       "6         0 2010-07-01   268.5300         5.596680        9.0   857.4000   \n",
       "7         0 2010-08-01   719.0300         6.579293       12.0   112.2200   \n",
       "8         0 2010-09-01   535.2000         6.284507       10.0   268.5300   \n",
       "9         0 2010-10-01  -211.9624        -5.361116        4.0   719.0300   \n",
       "10        0 2010-11-01   262.5600         5.574281       10.0   535.2000   \n",
       "11        0 2010-12-01   250.6300         5.527960       10.0  -211.9624   \n",
       "12        0 2011-01-01   153.6400         5.041100        9.0   262.5600   \n",
       "13        0 2011-02-01  1062.8000         6.969603        9.0   250.6300   \n",
       "14        0 2011-03-01  -210.7500        -5.355406       10.0   153.6400   \n",
       "15        0 2011-04-01   833.6100         6.726965       11.0  1062.8000   \n",
       "16        0 2011-05-01   493.2600         6.203062        8.0  -210.7500   \n",
       "17        0 2011-06-01   369.5800         5.915069        9.0   833.6100   \n",
       "18        0 2011-07-01   284.4900         5.654207       16.0   493.2600   \n",
       "19        0 2011-08-01   424.4400         6.053124        9.0   369.5800   \n",
       "\n",
       "    lag_amt_trans_3  lag_cnt_3  lag_amt_6  lag_amt_trans_6  lag_cnt_6  \\\n",
       "0               NaN        NaN        NaN              NaN        NaN   \n",
       "1               NaN        NaN        NaN              NaN        NaN   \n",
       "2               NaN        NaN        NaN              NaN        NaN   \n",
       "3          6.306367       10.0        NaN              NaN        NaN   \n",
       "4          6.763642       11.0        NaN              NaN        NaN   \n",
       "5          6.664473       14.0        NaN              NaN        NaN   \n",
       "6          6.755070        8.0   547.0500         6.306367       10.0   \n",
       "7          4.729333        5.0   864.7900         6.763642       11.0   \n",
       "8          5.596680        9.0   783.0500         6.664473       14.0   \n",
       "9          6.579293       12.0   857.4000         6.755070        8.0   \n",
       "10         6.284507       10.0   112.2200         4.729333        5.0   \n",
       "11        -5.361116        4.0   268.5300         5.596680        9.0   \n",
       "12         5.574281       10.0   719.0300         6.579293       12.0   \n",
       "13         5.527960       10.0   535.2000         6.284507       10.0   \n",
       "14         5.041100        9.0  -211.9624        -5.361116        4.0   \n",
       "15         6.969603        9.0   262.5600         5.574281       10.0   \n",
       "16        -5.355406       10.0   250.6300         5.527960       10.0   \n",
       "17         6.726965       11.0   153.6400         5.041100        9.0   \n",
       "18         6.203062        8.0  1062.8000         6.969603        9.0   \n",
       "19         5.915069        9.0  -210.7500        -5.355406       10.0   \n",
       "\n",
       "    lag_amt_12  lag_amt_trans_12  lag_cnt_12  lag_amt_12  lag_amt_trans_12  \\\n",
       "0          NaN               NaN         NaN         NaN               NaN   \n",
       "1          NaN               NaN         NaN         NaN               NaN   \n",
       "2          NaN               NaN         NaN         NaN               NaN   \n",
       "3          NaN               NaN         NaN         NaN               NaN   \n",
       "4          NaN               NaN         NaN         NaN               NaN   \n",
       "5          NaN               NaN         NaN         NaN               NaN   \n",
       "6          NaN               NaN         NaN         NaN               NaN   \n",
       "7          NaN               NaN         NaN         NaN               NaN   \n",
       "8          NaN               NaN         NaN         NaN               NaN   \n",
       "9          NaN               NaN         NaN         NaN               NaN   \n",
       "10         NaN               NaN         NaN         NaN               NaN   \n",
       "11         NaN               NaN         NaN         NaN               NaN   \n",
       "12      547.05          6.306367        10.0      547.05          6.306367   \n",
       "13      864.79          6.763642        11.0      864.79          6.763642   \n",
       "14      783.05          6.664473        14.0      783.05          6.664473   \n",
       "15      857.40          6.755070         8.0      857.40          6.755070   \n",
       "16      112.22          4.729333         5.0      112.22          4.729333   \n",
       "17      268.53          5.596680         9.0      268.53          5.596680   \n",
       "18      719.03          6.579293        12.0      719.03          6.579293   \n",
       "19      535.20          6.284507        10.0      535.20          6.284507   \n",
       "\n",
       "    s_diff_12  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "5         NaN  \n",
       "6         NaN  \n",
       "7         NaN  \n",
       "8         NaN  \n",
       "9         NaN  \n",
       "10        NaN  \n",
       "11        NaN  \n",
       "12     515.75  \n",
       "13   -1075.54  \n",
       "14      50.56  \n",
       "15    -364.14  \n",
       "16     257.36  \n",
       "17      15.96  \n",
       "18    -294.59  \n",
       "19     436.15  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === LAG FEATURES ===\n",
    "# Configurable lag list; default compact set\n",
    "LAGS = [1, 3, 6, 12]\n",
    "SEASONAL_LAGS = [12, 24]  # include 24 only when at least 2 years of data per user\n",
    "USE_COUNT_LAGS = True\n",
    "COMPUTE_SEASONAL_DIFFS = True\n",
    "\n",
    "# Ensure monthly is sorted\n",
    "monthly = monthly.sort_values(['user_id','month']).reset_index(drop=True)\n",
    "\n",
    "# Amount lags on winsorized amount and transformed amount (amt_t_trans)\n",
    "for lag in sorted(set(LAGS + SEASONAL_LAGS)):\n",
    "    monthly[f'lag_amt_{lag}'] = monthly.groupby('user_id')['amt_t_wins'].shift(lag)\n",
    "    if 'amt_t_trans' in monthly.columns:\n",
    "        monthly[f'lag_amt_trans_{lag}'] = monthly.groupby('user_id')['amt_t_trans'].shift(lag)\n",
    "\n",
    "# Count lags\n",
    "if USE_COUNT_LAGS:\n",
    "    for lag in LAGS:\n",
    "        monthly[f'lag_cnt_{lag}'] = monthly.groupby('user_id')['txn_cnt_t'].shift(lag)\n",
    "\n",
    "# Seasonal differences (amt_t - amt_{t-12}) — also provide on transformed scale if available\n",
    "if COMPUTE_SEASONAL_DIFFS:\n",
    "    monthly['s_diff_12'] = monthly['amt_t_wins'] - monthly.groupby('user_id')['amt_t_wins'].shift(12)\n",
    "    if 'amt_t_trans' in monthly.columns:\n",
    "        monthly['s_diff_12_trans'] = monthly['amt_t_trans'] - monthly.groupby('user_id')['amt_t_trans'].shift(12)\n",
    "\n",
    "# Feature width guard: if you want to reduce feature count, we can keep only LAGS + seasonal 12\n",
    "compact_cols = []\n",
    "for lag in LAGS:\n",
    "    compact_cols.append(f'lag_amt_{lag}')\n",
    "    if 'amt_t_trans' in monthly.columns:\n",
    "        compact_cols.append(f'lag_amt_trans_{lag}')\n",
    "    if USE_COUNT_LAGS:\n",
    "        compact_cols.append(f'lag_cnt_{lag}')\n",
    "# always include lag_12 for seasonality\n",
    "compact_cols += [f'lag_amt_12']\n",
    "if 'amt_t_trans' in monthly.columns:\n",
    "    compact_cols += [f'lag_amt_trans_12']\n",
    "\n",
    "print('Created lag columns; sample columns:', [c for c in monthly.columns if c.startswith('lag_')][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "870976a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added rolling features: 31 columns\n",
      "['roll_mean_3', 'roll_sum_3', 'roll_std_3', 'roll_cv_3', 'roll_mean_trans_3', 'roll_std_trans_3', 'roll_cv_trans_3', 'roll_mean_6', 'roll_sum_6', 'roll_std_6', 'roll_cv_6', 'roll_mean_trans_6', 'roll_std_trans_6', 'roll_cv_trans_6', 'roll_mean_12', 'roll_sum_12', 'roll_std_12', 'roll_cv_12', 'roll_mean_trans_12', 'roll_std_trans_12', 'roll_cv_trans_12', 'mom_1', 'mom_3', 'accel', 'z_6', 'ewm_mean_a50', 'ewm_std_a50', 'ewm_mean_trans_a50', 'ewm_std_trans_a50', 'expanding_mean', 'expanding_count']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>amt_t_wins</th>\n",
       "      <th>roll_mean_3</th>\n",
       "      <th>roll_sum_3</th>\n",
       "      <th>roll_std_3</th>\n",
       "      <th>roll_cv_3</th>\n",
       "      <th>roll_mean_trans_3</th>\n",
       "      <th>roll_std_trans_3</th>\n",
       "      <th>roll_cv_trans_3</th>\n",
       "      <th>...</th>\n",
       "      <th>roll_cv_6</th>\n",
       "      <th>roll_mean_trans_6</th>\n",
       "      <th>roll_std_trans_6</th>\n",
       "      <th>roll_cv_trans_6</th>\n",
       "      <th>roll_mean_12</th>\n",
       "      <th>roll_sum_12</th>\n",
       "      <th>roll_std_12</th>\n",
       "      <th>roll_cv_12</th>\n",
       "      <th>roll_mean_trans_12</th>\n",
       "      <th>roll_std_trans_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>547.050000</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>547.050000</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.306367</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>705.920000</td>\n",
       "      <td>1411.8400</td>\n",
       "      <td>158.870000</td>\n",
       "      <td>0.225054</td>\n",
       "      <td>6.535004</td>\n",
       "      <td>0.228638</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225054</td>\n",
       "      <td>6.535004</td>\n",
       "      <td>0.228638</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>705.920000</td>\n",
       "      <td>1411.8400</td>\n",
       "      <td>158.870000</td>\n",
       "      <td>0.225054</td>\n",
       "      <td>6.535004</td>\n",
       "      <td>0.228638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>731.630000</td>\n",
       "      <td>2194.8900</td>\n",
       "      <td>134.716218</td>\n",
       "      <td>0.184132</td>\n",
       "      <td>6.578161</td>\n",
       "      <td>0.196405</td>\n",
       "      <td>0.029857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184132</td>\n",
       "      <td>6.578161</td>\n",
       "      <td>0.196405</td>\n",
       "      <td>0.029857</td>\n",
       "      <td>731.630000</td>\n",
       "      <td>2194.8900</td>\n",
       "      <td>134.716218</td>\n",
       "      <td>0.184132</td>\n",
       "      <td>6.578161</td>\n",
       "      <td>0.196405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>835.080000</td>\n",
       "      <td>2505.2400</td>\n",
       "      <td>36.914258</td>\n",
       "      <td>0.044204</td>\n",
       "      <td>6.727728</td>\n",
       "      <td>0.044865</td>\n",
       "      <td>0.006669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168729</td>\n",
       "      <td>6.622388</td>\n",
       "      <td>0.186546</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>763.072500</td>\n",
       "      <td>3052.2900</td>\n",
       "      <td>128.752619</td>\n",
       "      <td>0.168729</td>\n",
       "      <td>6.622388</td>\n",
       "      <td>0.186546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>584.223333</td>\n",
       "      <td>1752.6700</td>\n",
       "      <td>335.134143</td>\n",
       "      <td>0.573640</td>\n",
       "      <td>6.049625</td>\n",
       "      <td>0.934320</td>\n",
       "      <td>0.154443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449791</td>\n",
       "      <td>6.243777</td>\n",
       "      <td>0.775387</td>\n",
       "      <td>0.124186</td>\n",
       "      <td>632.902000</td>\n",
       "      <td>3164.5100</td>\n",
       "      <td>284.673894</td>\n",
       "      <td>0.449791</td>\n",
       "      <td>6.243777</td>\n",
       "      <td>0.775387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>412.716667</td>\n",
       "      <td>1238.1500</td>\n",
       "      <td>320.848515</td>\n",
       "      <td>0.777406</td>\n",
       "      <td>5.693694</td>\n",
       "      <td>0.829844</td>\n",
       "      <td>0.145748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512451</td>\n",
       "      <td>6.135927</td>\n",
       "      <td>0.747782</td>\n",
       "      <td>0.121869</td>\n",
       "      <td>572.173333</td>\n",
       "      <td>3433.0400</td>\n",
       "      <td>293.210748</td>\n",
       "      <td>0.512451</td>\n",
       "      <td>6.135927</td>\n",
       "      <td>0.747782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>366.593333</td>\n",
       "      <td>1099.7800</td>\n",
       "      <td>257.250730</td>\n",
       "      <td>0.701733</td>\n",
       "      <td>5.635102</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495518</td>\n",
       "      <td>6.181415</td>\n",
       "      <td>0.764872</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>593.152857</td>\n",
       "      <td>4152.0700</td>\n",
       "      <td>276.281729</td>\n",
       "      <td>0.465785</td>\n",
       "      <td>6.199265</td>\n",
       "      <td>0.709483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>507.586667</td>\n",
       "      <td>1522.7600</td>\n",
       "      <td>184.949425</td>\n",
       "      <td>0.364370</td>\n",
       "      <td>6.153493</td>\n",
       "      <td>0.411708</td>\n",
       "      <td>0.066906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500757</td>\n",
       "      <td>6.101559</td>\n",
       "      <td>0.723827</td>\n",
       "      <td>0.118630</td>\n",
       "      <td>585.908750</td>\n",
       "      <td>4687.2700</td>\n",
       "      <td>259.147611</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>6.209921</td>\n",
       "      <td>0.664259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>347.422533</td>\n",
       "      <td>1042.2676</td>\n",
       "      <td>402.601536</td>\n",
       "      <td>1.158824</td>\n",
       "      <td>2.500895</td>\n",
       "      <td>5.560583</td>\n",
       "      <td>2.223437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961635</td>\n",
       "      <td>4.097295</td>\n",
       "      <td>4.284023</td>\n",
       "      <td>1.045574</td>\n",
       "      <td>497.256400</td>\n",
       "      <td>4475.3076</td>\n",
       "      <td>350.099207</td>\n",
       "      <td>0.704062</td>\n",
       "      <td>4.924250</td>\n",
       "      <td>3.689960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>195.265867</td>\n",
       "      <td>585.7976</td>\n",
       "      <td>308.717010</td>\n",
       "      <td>1.581009</td>\n",
       "      <td>2.165891</td>\n",
       "      <td>5.330289</td>\n",
       "      <td>2.461015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056433</td>\n",
       "      <td>3.900496</td>\n",
       "      <td>4.183349</td>\n",
       "      <td>1.072517</td>\n",
       "      <td>473.786760</td>\n",
       "      <td>4737.8676</td>\n",
       "      <td>339.514249</td>\n",
       "      <td>0.716597</td>\n",
       "      <td>4.989253</td>\n",
       "      <td>3.506031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      month  amt_t_wins  roll_mean_3  roll_sum_3  roll_std_3  \\\n",
       "0        0 2010-01-01    547.0500   547.050000    547.0500    0.000000   \n",
       "1        0 2010-02-01    864.7900   705.920000   1411.8400  158.870000   \n",
       "2        0 2010-03-01    783.0500   731.630000   2194.8900  134.716218   \n",
       "3        0 2010-04-01    857.4000   835.080000   2505.2400   36.914258   \n",
       "4        0 2010-05-01    112.2200   584.223333   1752.6700  335.134143   \n",
       "5        0 2010-06-01    268.5300   412.716667   1238.1500  320.848515   \n",
       "6        0 2010-07-01    719.0300   366.593333   1099.7800  257.250730   \n",
       "7        0 2010-08-01    535.2000   507.586667   1522.7600  184.949425   \n",
       "8        0 2010-09-01   -211.9624   347.422533   1042.2676  402.601536   \n",
       "9        0 2010-10-01    262.5600   195.265867    585.7976  308.717010   \n",
       "\n",
       "   roll_cv_3  roll_mean_trans_3  roll_std_trans_3  roll_cv_trans_3  ...  \\\n",
       "0   0.000000           6.306367          0.000000         0.000000  ...   \n",
       "1   0.225054           6.535004          0.228638         0.034987  ...   \n",
       "2   0.184132           6.578161          0.196405         0.029857  ...   \n",
       "3   0.044204           6.727728          0.044865         0.006669  ...   \n",
       "4   0.573640           6.049625          0.934320         0.154443  ...   \n",
       "5   0.777406           5.693694          0.829844         0.145748  ...   \n",
       "6   0.701733           5.635102          0.755732         0.134111  ...   \n",
       "7   0.364370           6.153493          0.411708         0.066906  ...   \n",
       "8   1.158824           2.500895          5.560583         2.223437  ...   \n",
       "9   1.581009           2.165891          5.330289         2.461015  ...   \n",
       "\n",
       "   roll_cv_6  roll_mean_trans_6  roll_std_trans_6  roll_cv_trans_6  \\\n",
       "0   0.000000           6.306367          0.000000         0.000000   \n",
       "1   0.225054           6.535004          0.228638         0.034987   \n",
       "2   0.184132           6.578161          0.196405         0.029857   \n",
       "3   0.168729           6.622388          0.186546         0.028169   \n",
       "4   0.449791           6.243777          0.775387         0.124186   \n",
       "5   0.512451           6.135927          0.747782         0.121869   \n",
       "6   0.495518           6.181415          0.764872         0.123737   \n",
       "7   0.500757           6.101559          0.723827         0.118630   \n",
       "8   0.961635           4.097295          4.284023         1.045574   \n",
       "9   1.056433           3.900496          4.183349         1.072517   \n",
       "\n",
       "   roll_mean_12  roll_sum_12  roll_std_12  roll_cv_12  roll_mean_trans_12  \\\n",
       "0    547.050000     547.0500     0.000000    0.000000            6.306367   \n",
       "1    705.920000    1411.8400   158.870000    0.225054            6.535004   \n",
       "2    731.630000    2194.8900   134.716218    0.184132            6.578161   \n",
       "3    763.072500    3052.2900   128.752619    0.168729            6.622388   \n",
       "4    632.902000    3164.5100   284.673894    0.449791            6.243777   \n",
       "5    572.173333    3433.0400   293.210748    0.512451            6.135927   \n",
       "6    593.152857    4152.0700   276.281729    0.465785            6.199265   \n",
       "7    585.908750    4687.2700   259.147611    0.442300            6.209921   \n",
       "8    497.256400    4475.3076   350.099207    0.704062            4.924250   \n",
       "9    473.786760    4737.8676   339.514249    0.716597            4.989253   \n",
       "\n",
       "   roll_std_trans_12  \n",
       "0           0.000000  \n",
       "1           0.228638  \n",
       "2           0.196405  \n",
       "3           0.186546  \n",
       "4           0.775387  \n",
       "5           0.747782  \n",
       "6           0.709483  \n",
       "7           0.664259  \n",
       "8           3.689960  \n",
       "9           3.506031  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ROLLING & EXPANDING FEATURES ===\n",
    "# Fixed-width rolling windows (means, sums, stds, cv), momentum, acceleration, z-score, and EWM stats.\n",
    "# Uses data ≤ t (rolling windows include current row \"t\").\n",
    "\n",
    "# Parameters\n",
    "ROLL_WINDOWS = [3, 6, 12]   # short, medium, long memory\n",
    "EWM_ALPHAS = [0.5]          # list of alphas for EWM; default 0.5 (fast reaction)\n",
    "EPS = 1e-8\n",
    "ROLL_DDF = 0                # ddof for std (0 -> population std, avoids NaN for single obs)\n",
    "MIN_PERIODS = 1             # allow shorter windows at start\n",
    "\n",
    "# Ensure sorted\n",
    "monthly = monthly.sort_values(['user_id', 'month']).reset_index(drop=True)\n",
    "\n",
    "rolling_cols = []\n",
    "ewm_cols = []\n",
    "ewm_std_cols = []\n",
    "\n",
    "# Rolling stats on winsorized amount\n",
    "for w in ROLL_WINDOWS:\n",
    "    roll_mean_col = f'roll_mean_{w}'\n",
    "    roll_sum_col = f'roll_sum_{w}'\n",
    "    roll_std_col = f'roll_std_{w}'\n",
    "    monthly[roll_mean_col] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.rolling(window=w, min_periods=MIN_PERIODS).mean())\n",
    "    monthly[roll_sum_col] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.rolling(window=w, min_periods=MIN_PERIODS).sum())\n",
    "    # use ddof=0 to avoid NaN for single-value windows\n",
    "    monthly[roll_std_col] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.rolling(window=w, min_periods=MIN_PERIODS).std(ddof=ROLL_DDF)).fillna(0)\n",
    "    monthly[f'roll_cv_{w}'] = monthly[roll_std_col] / (monthly[roll_mean_col] + EPS)\n",
    "\n",
    "    rolling_cols += [roll_mean_col, roll_sum_col, roll_std_col, f'roll_cv_{w}']\n",
    "\n",
    "    # also compute on transformed amount if available\n",
    "    if 'amt_t_trans' in monthly.columns:\n",
    "        roll_mean_t = f'roll_mean_trans_{w}'\n",
    "        roll_std_t = f'roll_std_trans_{w}'\n",
    "        monthly[roll_mean_t] = monthly.groupby('user_id')['amt_t_trans'].transform(lambda x: x.rolling(window=w, min_periods=MIN_PERIODS).mean())\n",
    "        monthly[roll_std_t] = monthly.groupby('user_id')['amt_t_trans'].transform(lambda x: x.rolling(window=w, min_periods=MIN_PERIODS).std(ddof=ROLL_DDF)).fillna(0)\n",
    "        monthly[f'roll_cv_trans_{w}'] = monthly[roll_std_t] / (monthly[roll_mean_t].abs() + EPS)\n",
    "        rolling_cols += [roll_mean_t, roll_std_t, f'roll_cv_trans_{w}']\n",
    "\n",
    "# Momentum & acceleration\n",
    "monthly['mom_1'] = monthly['amt_t_wins'] - monthly.groupby('user_id')['amt_t_wins'].shift(1)\n",
    "monthly['mom_3'] = None\n",
    "if 3 in ROLL_WINDOWS:\n",
    "    # mom_3 defined as amt_t - mean(amt_{t-2..t}) --- uses the roll_mean_3 computed above\n",
    "    monthly['mom_3'] = monthly['amt_t_wins'] - monthly['roll_mean_3']\n",
    "else:\n",
    "    monthly['mom_3'] = monthly['amt_t_wins'] - monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.rolling(window=3, min_periods=MIN_PERIODS).mean())\n",
    "\n",
    "# accel = mom_1 - (amt_{t-1} - amt_{t-2})\n",
    "prev_diff = monthly.groupby('user_id')['amt_t_wins'].shift(1) - monthly.groupby('user_id')['amt_t_wins'].shift(2)\n",
    "monthly['accel'] = monthly['mom_1'] - prev_diff\n",
    "rolling_cols += ['mom_1', 'mom_3', 'accel']\n",
    "\n",
    "# Relative position (z-score)\n",
    "if 6 in ROLL_WINDOWS:\n",
    "    monthly['z_6'] = (monthly['amt_t_wins'] - monthly['roll_mean_6']) / (monthly['roll_std_6'] + EPS)\n",
    "else:\n",
    "    monthly['z_6'] = (monthly['amt_t_wins'] - monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.rolling(window=6, min_periods=MIN_PERIODS).mean())) / (\n",
    "        monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.rolling(window=6, min_periods=MIN_PERIODS).std(ddof=ROLL_DDF)).fillna(0) + EPS\n",
    "    )\n",
    "rolling_cols += ['z_6']\n",
    "\n",
    "# EWM stats (fast-reacting)\n",
    "ewm_cols = []\n",
    "ewm_std_cols = []\n",
    "for alpha in EWM_ALPHAS:\n",
    "    suffix = f'a{int(alpha*100):02d}'\n",
    "    ewm_mean_col = f'ewm_mean_{suffix}'\n",
    "    ewm_std_col = f'ewm_std_{suffix}'\n",
    "    monthly[ewm_mean_col] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.ewm(alpha=alpha, adjust=False).mean())\n",
    "    # ewm.std may produce NaN on first obs; fill with 0\n",
    "    monthly[ewm_std_col] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.ewm(alpha=alpha, adjust=False).std()).fillna(0)\n",
    "    rolling_cols += [ewm_mean_col, ewm_std_col]\n",
    "    if 'amt_t_trans' in monthly.columns:\n",
    "        emt = f'ewm_mean_trans_{suffix}'\n",
    "        est = f'ewm_std_trans_{suffix}'\n",
    "        monthly[emt] = monthly.groupby('user_id')['amt_t_trans'].transform(lambda x: x.ewm(alpha=alpha, adjust=False).mean())\n",
    "        monthly[est] = monthly.groupby('user_id')['amt_t_trans'].transform(lambda x: x.ewm(alpha=alpha, adjust=False).std()).fillna(0)\n",
    "        rolling_cols += [emt, est]\n",
    "\n",
    "# Expanding stats (optional): expanding mean and count already exist in earlier steps as expanding_mean/expanding_count in supervised builder; include here if needed\n",
    "monthly['expanding_mean'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.expanding(min_periods=1).mean())\n",
    "monthly['expanding_count'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda x: x.expanding(min_periods=1).count())\n",
    "rolling_cols += ['expanding_mean', 'expanding_count']\n",
    "\n",
    "# Final housekeeping: ensure numeric dtypes and update core_transforms\n",
    "rolling_cols = [c for c in rolling_cols if c in monthly.columns]\n",
    "core_transforms['rolling_windows'] = ROLL_WINDOWS\n",
    "core_transforms['ewm_alphas'] = EWM_ALPHAS\n",
    "core_transforms['rolling_cols'] = rolling_cols\n",
    "core_transforms['rolling_eps'] = EPS\n",
    "\n",
    "print('Added rolling features:', len(rolling_cols), 'columns')\n",
    "print(rolling_cols[:80])\n",
    "monthly[['user_id','month','amt_t_wins'] + rolling_cols[:20]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ca010",
   "metadata": {},
   "source": [
    "# Expanding (cumulative) history features\n",
    "\n",
    "Compute per-user cumulative statistics up to and including time t (no leakage):\n",
    "\n",
    "- exp_mean, exp_sum, exp_std (cumulative mean, sum, std)\n",
    "- seasonal_mean_cum (mean for same month_of_year up to t) and seasonal_ratio = seasonal_mean_cum / exp_mean\n",
    "- behavioral persistence: nonzero_rate (fraction non-zero months up to t) and run_length_since_zero\n",
    "\n",
    "These features provide a long-run baseline and capture persistence in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db5cdfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added expanding (cumulative) features: ['exp_mean', 'exp_sum', 'exp_std', 'seasonal_mean_cum', 'seasonal_ratio', 'nonzero_cum', 'months_seen_cum', 'nonzero_rate', 'run_length_since_zero']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>month</th>\n",
       "      <th>exp_mean</th>\n",
       "      <th>exp_sum</th>\n",
       "      <th>exp_std</th>\n",
       "      <th>seasonal_mean_cum</th>\n",
       "      <th>seasonal_ratio</th>\n",
       "      <th>nonzero_cum</th>\n",
       "      <th>months_seen_cum</th>\n",
       "      <th>nonzero_rate</th>\n",
       "      <th>run_length_since_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>547.050000</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>547.0500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>705.920000</td>\n",
       "      <td>1411.8400</td>\n",
       "      <td>158.870000</td>\n",
       "      <td>864.7900</td>\n",
       "      <td>1.225054</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>731.630000</td>\n",
       "      <td>2194.8900</td>\n",
       "      <td>134.716218</td>\n",
       "      <td>783.0500</td>\n",
       "      <td>1.070281</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>763.072500</td>\n",
       "      <td>3052.2900</td>\n",
       "      <td>128.752619</td>\n",
       "      <td>857.4000</td>\n",
       "      <td>1.123615</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>632.902000</td>\n",
       "      <td>3164.5100</td>\n",
       "      <td>284.673894</td>\n",
       "      <td>112.2200</td>\n",
       "      <td>0.177310</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>572.173333</td>\n",
       "      <td>3433.0400</td>\n",
       "      <td>293.210748</td>\n",
       "      <td>268.5300</td>\n",
       "      <td>0.469316</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>593.152857</td>\n",
       "      <td>4152.0700</td>\n",
       "      <td>276.281729</td>\n",
       "      <td>719.0300</td>\n",
       "      <td>1.212217</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>585.908750</td>\n",
       "      <td>4687.2700</td>\n",
       "      <td>259.147611</td>\n",
       "      <td>535.2000</td>\n",
       "      <td>0.913453</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>497.256400</td>\n",
       "      <td>4475.3076</td>\n",
       "      <td>350.099207</td>\n",
       "      <td>-211.9624</td>\n",
       "      <td>-0.426264</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>473.786760</td>\n",
       "      <td>4737.8676</td>\n",
       "      <td>339.514249</td>\n",
       "      <td>262.5600</td>\n",
       "      <td>0.554173</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>453.499782</td>\n",
       "      <td>4988.4976</td>\n",
       "      <td>330.009782</td>\n",
       "      <td>250.6300</td>\n",
       "      <td>0.552657</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>428.511467</td>\n",
       "      <td>5142.1376</td>\n",
       "      <td>326.648883</td>\n",
       "      <td>153.6400</td>\n",
       "      <td>0.358543</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id      month    exp_mean    exp_sum     exp_std  seasonal_mean_cum  \\\n",
       "0         0 2010-01-01  547.050000   547.0500    0.000000           547.0500   \n",
       "1         0 2010-02-01  705.920000  1411.8400  158.870000           864.7900   \n",
       "2         0 2010-03-01  731.630000  2194.8900  134.716218           783.0500   \n",
       "3         0 2010-04-01  763.072500  3052.2900  128.752619           857.4000   \n",
       "4         0 2010-05-01  632.902000  3164.5100  284.673894           112.2200   \n",
       "5         0 2010-06-01  572.173333  3433.0400  293.210748           268.5300   \n",
       "6         0 2010-07-01  593.152857  4152.0700  276.281729           719.0300   \n",
       "7         0 2010-08-01  585.908750  4687.2700  259.147611           535.2000   \n",
       "8         0 2010-09-01  497.256400  4475.3076  350.099207          -211.9624   \n",
       "9         0 2010-10-01  473.786760  4737.8676  339.514249           262.5600   \n",
       "10        0 2010-11-01  453.499782  4988.4976  330.009782           250.6300   \n",
       "11        0 2010-12-01  428.511467  5142.1376  326.648883           153.6400   \n",
       "\n",
       "    seasonal_ratio  nonzero_cum  months_seen_cum  nonzero_rate  \\\n",
       "0         1.000000          1.0              1.0      1.000000   \n",
       "1         1.225054          2.0              2.0      1.000000   \n",
       "2         1.070281          3.0              3.0      1.000000   \n",
       "3         1.123615          4.0              4.0      1.000000   \n",
       "4         0.177310          5.0              5.0      1.000000   \n",
       "5         0.469316          6.0              6.0      1.000000   \n",
       "6         1.212217          7.0              7.0      1.000000   \n",
       "7         0.913453          8.0              8.0      1.000000   \n",
       "8        -0.426264          8.0              9.0      0.888889   \n",
       "9         0.554173          9.0             10.0      0.900000   \n",
       "10        0.552657         10.0             11.0      0.909091   \n",
       "11        0.358543         11.0             12.0      0.916667   \n",
       "\n",
       "    run_length_since_zero  \n",
       "0                       1  \n",
       "1                       2  \n",
       "2                       3  \n",
       "3                       4  \n",
       "4                       5  \n",
       "5                       6  \n",
       "6                       7  \n",
       "7                       8  \n",
       "8                       9  \n",
       "9                      10  \n",
       "10                     11  \n",
       "11                     12  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === EXPANDING / CUMULATIVE FEATURES ===\n",
    "# Cumulative (expanding) features per user up to time t.\n",
    "\n",
    "# Reuse EPS & ROLL_DDF if defined, else fall back\n",
    "EPS = globals().get('EPS', 1e-8)\n",
    "ROLL_DDF = globals().get('ROLL_DDF', 0)\n",
    "\n",
    "# Ensure sorted\n",
    "monthly = monthly.sort_values(['user_id','month']).reset_index(drop=True)\n",
    "\n",
    "# Cumulative mean, sum, std\n",
    "monthly['exp_mean'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda s: s.expanding(min_periods=1).mean())\n",
    "monthly['exp_sum'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda s: s.expanding(min_periods=1).sum())\n",
    "monthly['exp_std'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda s: s.expanding(min_periods=1).std(ddof=ROLL_DDF)).fillna(0)\n",
    "\n",
    "# Seasonal cumulative mean: mean of same month_of_year up to t\n",
    "monthly['seasonal_mean_cum'] = monthly.groupby(['user_id','month_of_year'])['amt_t_wins'].transform(lambda s: s.expanding(min_periods=1).mean())\n",
    "monthly['seasonal_ratio'] = monthly['seasonal_mean_cum'] / (monthly['exp_mean'] + EPS)\n",
    "monthly['seasonal_ratio'] = monthly['seasonal_ratio'].replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "\n",
    "# Behavioral persistence\n",
    "monthly['nonzero_cum'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda s: s.gt(0).expanding(min_periods=1).sum())\n",
    "monthly['months_seen_cum'] = monthly.groupby('user_id')['amt_t_wins'].transform(lambda s: s.expanding(min_periods=1).count())\n",
    "monthly['nonzero_rate'] = monthly['nonzero_cum'] / (monthly['months_seen_cum'] + EPS)\n",
    "\n",
    "# run_length_since_zero: months since last zero-spend month (0 if current month is zero)\n",
    "def _run_length_since_zero(s):\n",
    "    zm = s.eq(0)\n",
    "    gid = zm.cumsum()\n",
    "    out = gid.groupby(gid).cumcount().astype(int)\n",
    "    # for pre-zero group (gid == 0) and non-zero values, interpret as months since start\n",
    "    mask_pre_zero_nonzero = (~zm) & (gid == 0)\n",
    "    out[mask_pre_zero_nonzero] = out[mask_pre_zero_nonzero] + 1\n",
    "    out[zm] = 0\n",
    "    return out\n",
    "\n",
    "monthly['run_length_since_zero'] = monthly.groupby('user_id')['amt_t_wins'].transform(_run_length_since_zero)\n",
    "\n",
    "# Housekeeping\n",
    "exp_cols = ['exp_mean','exp_sum','exp_std','seasonal_mean_cum','seasonal_ratio','nonzero_cum','months_seen_cum','nonzero_rate','run_length_since_zero']\n",
    "core_transforms['expanding_cols'] = exp_cols\n",
    "\n",
    "print('Added expanding (cumulative) features:', exp_cols)\n",
    "monthly[['user_id','month'] + exp_cols].head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ffdfe",
   "metadata": {},
   "source": [
    "# Category / mix features (top-k shares, trends, entropy, weekend bias)\n",
    "\n",
    "If a category column exists (e.g., `category`, `mcc`), this cell will:\n",
    "\n",
    "- pick top-K categories from the TRAIN partition,\n",
    "- compute per-user-month shares for each top-K category and an 'other' share,\n",
    "- compute 3-month rolling averages of shares and change vs 12-month average,\n",
    "- compute category entropy per month (diversity of spending),\n",
    "- compute weekend_share per month (weekend vs weekday allocation).\n",
    "\n",
    "This is optional and skipped if no category-like column is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12c7f1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using category column: category\n",
      "Top-k categories (train): ['Food & Dining', 'Transportation & Travel', 'Shopping & Retail', 'Financial & Insurance', 'Home & Utilities']\n",
      "Added category/mix features: ['share_food_dining', 'share_transportation_travel', 'share_shopping_retail', 'share_financial_insurance', 'share_home_utilities', 'share_food_dining_roll3', 'share_transportation_travel_roll3', 'share_shopping_retail_roll3', 'share_financial_insurance_roll3', 'share_home_utilities_roll3', 'share_food_dining_chg_vs12', 'share_transportation_travel_chg_vs12', 'share_shopping_retail_chg_vs12', 'share_financial_insurance_chg_vs12', 'share_home_utilities_chg_vs12', 'share_other', 'cat_entropy', 'weekend_share']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cunbidun/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:395: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# === CATEGORY / MIX FEATURES ===\n",
    "import re\n",
    "\n",
    "# Candidate category-like columns in transactions\n",
    "cat_candidates = ['category', 'mcc', 'merchant_id', 'merchant_city', 'description']\n",
    "cat_col = next((c for c in cat_candidates if c in df.columns), None)\n",
    "TOP_K = 5\n",
    "\n",
    "if cat_col is None:\n",
    "    print('No category-like column found in transactions; skipping category/mix features.')\n",
    "else:\n",
    "    print('Using category column:', cat_col)\n",
    "    # Use TRAIN transactions to compute global top-K categories to avoid label leakage\n",
    "    train_tx = df[df['month'] < core_transforms['val_start']]\n",
    "    topk = train_tx[cat_col].value_counts().nlargest(TOP_K).index.tolist()\n",
    "    print('Top-k categories (train):', topk)\n",
    "\n",
    "    # per-user-month category amounts\n",
    "    cat_monthly = (\n",
    "        df\n",
    "        .groupby(['user_id','month', cat_col], as_index=False)\n",
    "        ['amount']\n",
    "        .sum()\n",
    "        .rename(columns={'amount':'amt_cat'})\n",
    "    )\n",
    "\n",
    "    # pivot only for top-k categories (others will be aggregated into 'other')\n",
    "    cat_top = cat_monthly[cat_monthly[cat_col].isin(topk)].copy()\n",
    "    cat_wide = cat_top.pivot_table(index=['user_id','month'], columns=cat_col, values='amt_cat', aggfunc='sum').fillna(0)\n",
    "    cat_wide = cat_wide.reset_index()\n",
    "\n",
    "    # merge top-k amounts into monthly\n",
    "    monthly = monthly.merge(cat_wide, on=['user_id','month'], how='left')\n",
    "    # fill NA for top-k columns with 0\n",
    "    for c in topk:\n",
    "        if c in monthly.columns:\n",
    "            monthly[c] = monthly[c].fillna(0)\n",
    "\n",
    "    # compute other amount as remainder\n",
    "    top_cols = [c for c in topk if c in monthly.columns]\n",
    "    monthly['amt_topk_sum'] = monthly[top_cols].sum(axis=1) if top_cols else 0.0\n",
    "    monthly['amt_other'] = (monthly['amt_t'] - monthly['amt_topk_sum']).clip(lower=0.0)\n",
    "\n",
    "    # safe denominator\n",
    "    denom = monthly['amt_t'].clip(lower=EPS)\n",
    "\n",
    "    # create sanitized share column names\n",
    "    def _sanitize(x):\n",
    "        s = re.sub(r'[^0-9a-zA-Z]+', '_', str(x)).lower()\n",
    "        return s[:40]\n",
    "\n",
    "    share_cols = []\n",
    "    for c in top_cols:\n",
    "        sc = f'share_{_sanitize(c)}'\n",
    "        monthly[sc] = monthly[c] / denom\n",
    "        share_cols.append(sc)\n",
    "        # rolling mean and change vs 12-month average\n",
    "        monthly[f'{sc}_roll3'] = monthly.groupby('user_id')[sc].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
    "        monthly[f'{sc}_chg_vs12'] = monthly[sc] - monthly.groupby('user_id')[sc].transform(lambda s: s.rolling(12, min_periods=1).mean())\n",
    "\n",
    "    # other share\n",
    "    monthly['share_other'] = monthly['amt_other'] / denom\n",
    "\n",
    "    # entropy over top_k + other\n",
    "    ent_cols = share_cols + ['share_other']\n",
    "    monthly[ent_cols] = monthly[ent_cols].fillna(0)\n",
    "    monthly['cat_entropy'] = -(monthly[ent_cols] * np.log(monthly[ent_cols] + EPS)).sum(axis=1)\n",
    "\n",
    "    # weekend vs weekday bias\n",
    "    df['is_weekend'] = df['date'].dt.weekday >= 5\n",
    "    weekend_sum = (\n",
    "        df[df['is_weekend']]\n",
    "        .groupby(['user_id','month'], as_index=False)['amount']\n",
    "        .sum()\n",
    "        .rename(columns={'amount':'amt_weekend'})\n",
    "    )\n",
    "    monthly = monthly.merge(weekend_sum, on=['user_id','month'], how='left')\n",
    "    monthly['amt_weekend'] = monthly['amt_weekend'].fillna(0)\n",
    "    monthly['weekend_share'] = monthly['amt_weekend'] / denom\n",
    "\n",
    "    # housekeeping: collect feature names\n",
    "    cat_feature_cols = share_cols + [f'{c}_roll3' for c in share_cols] + [f'{c}_chg_vs12' for c in share_cols] + ['share_other','cat_entropy','weekend_share']\n",
    "    core_transforms['category_topk'] = topk\n",
    "    core_transforms['category_feature_cols'] = cat_feature_cols\n",
    "\n",
    "    print('Added category/mix features:', cat_feature_cols)\n",
    "    monthly[['user_id','month','amt_t'] + cat_feature_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54558bb",
   "metadata": {},
   "source": [
    "# Static (time-invariant) user features\n",
    "\n",
    "Add demographic and static user-level features that are repeated for every month for each user:\n",
    "\n",
    "- age_bucket (computed from birth year or current age)\n",
    "- income_bucket (quantile buckets computed on TRAIN users only)\n",
    "- region (derived from coordinates via KMeans on TRAIN users or extracted from address)\n",
    "- account_tenure_months (months since user start date at each t)\n",
    "\n",
    "These are stable or monotone features that help models learn cross-sectional differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d588a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available static columns: ['user_current_age', 'user_birth_year', 'user_birth_month', 'user_yearly_income', 'user_per_capita_income', 'user_gender', 'user_total_debt', 'user_credit_score', 'user_num_credit_cards', 'user_address', 'user_latitude', 'user_longitude', 'zip', 'card_acct_open_date']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'Timestamp' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m user_static[\u001b[33m'\u001b[39m\u001b[33mfirst_tx_date\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(user_static[\u001b[33m'\u001b[39m\u001b[33mfirst_tx_date\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     29\u001b[39m user_static[\u001b[33m'\u001b[39m\u001b[33mcard_acct_open_date\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(user_static.get(\u001b[33m'\u001b[39m\u001b[33mcard_acct_open_date\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m user_static[\u001b[33m'\u001b[39m\u001b[33muser_start_date\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43muser_static\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfirst_tx_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcard_acct_open_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# AGE: compute age at each month if birth year available, else use user_current_age as static\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33muser_birth_year\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m user_static.columns:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# merge birth year into monthly to compute age at each month\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/frame.py:11670\u001b[39m, in \u001b[36mDataFrame.min\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11662\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11663\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(\n\u001b[32m  11664\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11668\u001b[39m     **kwargs,\n\u001b[32m  11669\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11670\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11671\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11672\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/generic.py:12453\u001b[39m, in \u001b[36mNDFrame.min\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(\n\u001b[32m  12447\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12448\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12451\u001b[39m     **kwargs,\n\u001b[32m  12452\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m12453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12454\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  12455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12456\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12459\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12460\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/generic.py:12442\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12438\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12440\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/frame.py:11589\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11585\u001b[39m     df = df.T\n\u001b[32m  11587\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11588\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11589\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11590\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:1519\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1517\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1518\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1522\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:406\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    409\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/frame.py:11508\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11506\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/pandas/core/nanops.py:1098\u001b[39m, in \u001b[36m_nanminmax.<locals>.reduction\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _na_for_min_count(values, axis)\n\u001b[32m   1095\u001b[39m values, mask = _get_values(\n\u001b[32m   1096\u001b[39m     values, skipna, fill_value_typ=fill_value_typ, mask=mask\n\u001b[32m   1097\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1099\u001b[39m result = _maybe_null_out(result, axis, mask, values.shape)\n\u001b[32m   1100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Vi/Columbia University/Capstone/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:47\u001b[39m, in \u001b[36m_amin\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     46\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: '<=' not supported between instances of 'Timestamp' and 'float'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Candidate static columns to pull from df\n",
    "static_candidates = [\n",
    "    'user_current_age', 'user_birth_year', 'user_birth_month',\n",
    "    'user_yearly_income', 'user_per_capita_income', 'user_gender',\n",
    "    'user_total_debt', 'user_credit_score', 'user_num_credit_cards',\n",
    "    'user_address', 'user_latitude', 'user_longitude', 'zip', 'card_acct_open_date'\n",
    "]\n",
    "available_static = [c for c in static_candidates if c in df.columns]\n",
    "print('Available static columns:', available_static)\n",
    "\n",
    "# Build a per-user static table (last-known values)\n",
    "user_static = df.sort_values('date').drop_duplicates('user_id', keep='last')[['user_id'] + available_static].copy()\n",
    "\n",
    "# add first-transaction date per user (useful for tenure)\n",
    "user_first_tx = df.groupby('user_id', as_index=False)['date'].min().rename(columns={'date':'first_tx_date'})\n",
    "user_static = user_static.merge(user_first_tx, on='user_id', how='left')\n",
    "\n",
    "# card account open date (if available) - take earliest per user\n",
    "if 'card_acct_open_date' in df.columns:\n",
    "    card_dates = df[['user_id','card_acct_open_date']].dropna().copy()\n",
    "    card_dates['card_acct_open_date'] = pd.to_datetime(card_dates['card_acct_open_date'], errors='coerce')\n",
    "    user_card_open = card_dates.groupby('user_id', as_index=False)['card_acct_open_date'].min()\n",
    "    user_static = user_static.merge(user_card_open, on='user_id', how='left')\n",
    "else:\n",
    "    user_static['card_acct_open_date'] = pd.NaT\n",
    "\n",
    "# determine a conservative start date per user: earliest of first_tx_date and card_acct_open_date\n",
    "# Ensure both columns are datetimelike and coerce invalid values to NaT before taking row-wise min\n",
    "user_static['first_tx_date'] = pd.to_datetime(user_static.get('first_tx_date'), errors='coerce')\n",
    "user_static['card_acct_open_date'] = pd.to_datetime(user_static.get('card_acct_open_date'), errors='coerce')\n",
    "user_static['user_start_date'] = user_static[['first_tx_date','card_acct_open_date']].min(axis=1)\n",
    "\n",
    "# AGE: compute age at each month if birth year available, else use user_current_age as static\n",
    "if 'user_birth_year' in user_static.columns:\n",
    "    # merge birth year into monthly to compute age at each month\n",
    "    monthly = monthly.merge(user_static[['user_id','user_birth_year']], on='user_id', how='left')\n",
    "    monthly['age_at_t'] = monthly['month'].dt.year - monthly['user_birth_year']\n",
    "else:\n",
    "    if 'user_current_age' in user_static.columns:\n",
    "        monthly = monthly.merge(user_static[['user_id','user_current_age']], on='user_id', how='left')\n",
    "        monthly['age_at_t'] = monthly['user_current_age']\n",
    "    else:\n",
    "        monthly['age_at_t'] = np.nan\n",
    "\n",
    "# Age bucket (fixed bins)\n",
    "AGE_BINS = [0,25,35,45,55,65,200]\n",
    "AGE_LABELS = ['<25','25-34','35-44','45-54','55-64','65+']\n",
    "monthly['age_bucket'] = pd.cut(monthly['age_at_t'], bins=AGE_BINS, labels=AGE_LABELS, include_lowest=True).astype(object).fillna('unknown')\n",
    "\n",
    "# INCOME: choose yearly income then per-capita as fallback\n",
    "income_col = 'user_yearly_income' if 'user_yearly_income' in user_static.columns else ('user_per_capita_income' if 'user_per_capita_income' in user_static.columns else None)\n",
    "if income_col is None:\n",
    "    monthly['income_bucket'] = 'income_unknown'\n",
    "    user_static['income'] = np.nan\n",
    "else:\n",
    "    user_static['income'] = user_static[income_col]\n",
    "    # compute quantile thresholds on TRAIN USERS only\n",
    "    train_user_ids = monthly.loc[train_mask, 'user_id'].unique()\n",
    "    train_incomes = user_static.loc[user_static['user_id'].isin(train_user_ids), 'income'].dropna()\n",
    "    if train_incomes.nunique() >= 5:\n",
    "        # robust quantile bins (20% increments)\n",
    "        probs = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        quantile_vals = np.unique(train_incomes.quantile(probs).values)\n",
    "        if len(quantile_vals) >= 3:\n",
    "            bins = quantile_vals\n",
    "            labels = [f'inc_q{i+1}' for i in range(len(bins)-1)]\n",
    "            user_static['income_bucket'] = pd.cut(user_static['income'], bins=bins, labels=labels, include_lowest=True).astype(object)\n",
    "        else:\n",
    "            # fallback to median split\n",
    "            med = train_incomes.median()\n",
    "            user_static['income_bucket'] = np.where(user_static['income'] >= med, 'inc_high', 'inc_low')\n",
    "    elif train_incomes.nunique() >= 2:\n",
    "        med = train_incomes.median()\n",
    "        user_static['income_bucket'] = np.where(user_static['income'] >= med, 'inc_high', 'inc_low')\n",
    "    else:\n",
    "        user_static['income_bucket'] = 'income_unknown'\n",
    "\n",
    "    user_static['income_bucket'] = user_static['income_bucket'].fillna('income_unknown')\n",
    "    monthly = monthly.merge(user_static[['user_id','income','income_bucket']], on='user_id', how='left')\n",
    "\n",
    "# REGION: use lat/lon clustering if available, otherwise try to extract state from address, else unknown\n",
    "if 'user_latitude' in user_static.columns and 'user_longitude' in user_static.columns:\n",
    "    coords = user_static[['user_id','user_latitude','user_longitude']].dropna()\n",
    "    # fit KMeans on TRAIN users with coordinates\n",
    "    train_user_ids = monthly.loc[train_mask, 'user_id'].unique()\n",
    "    coord_train = coords[coords['user_id'].isin(train_user_ids)]\n",
    "    n_unique_coords = coord_train[['user_latitude','user_longitude']].drop_duplicates().shape[0]\n",
    "    N_REGIONS = 4\n",
    "    if n_unique_coords >= 2:\n",
    "        n_clusters = min(N_REGIONS, n_unique_coords)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        kmeans.fit(coord_train[['user_latitude','user_longitude']])\n",
    "        # assign cluster for all users with coords\n",
    "        coords['region_cluster'] = kmeans.predict(coords[['user_latitude','user_longitude']])\n",
    "        coords['region'] = coords['region_cluster'].apply(lambda x: f'region_{x}')\n",
    "        # merge region into user_static\n",
    "        user_static = user_static.merge(coords[['user_id','region']], on='user_id', how='left')\n",
    "        core_transforms['region_kmeans'] = kmeans\n",
    "    else:\n",
    "        user_static['region'] = 'region_unknown'\n",
    "else:\n",
    "    # try to extract state from 'user_address' if present\n",
    "    if 'user_address' in user_static.columns:\n",
    "        # naive extraction: look for two-letter state code before ZIP or at the end\n",
    "        def _extract_state(addr):\n",
    "            if pd.isna(addr):\n",
    "                return None\n",
    "            # common patterns: 'City, ST 12345' or 'City, ST'\n",
    "            m = re.search(r',\\s*([A-Z]{2})(?:\\s|$)', str(addr))\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "            return None\n",
    "        user_static['region'] = user_static['user_address'].apply(_extract_state).fillna('region_unknown')\n",
    "    else:\n",
    "        user_static['region'] = 'region_unknown'\n",
    "\n",
    "# Merge region into monthly (if not already)\n",
    "if 'region' not in monthly.columns:\n",
    "    monthly = monthly.merge(user_static[['user_id','region']], on='user_id', how='left')\n",
    "\n",
    "# ACCOUNT TENURE (months since user start) — compute per month t\n",
    "# ensure user_start_date exists\n",
    "user_static['user_start_date'] = pd.to_datetime(user_static.get('user_start_date'), errors='coerce')\n",
    "\n",
    "monthly = monthly.merge(user_static[['user_id','user_start_date']], on='user_id', how='left')\n",
    "monthly['user_start_date'] = pd.to_datetime(monthly['user_start_date'], errors='coerce')\n",
    "# compute months difference vectorized\n",
    "start_month_num = monthly['user_start_date'].dt.year * 12 + monthly['user_start_date'].dt.month\n",
    "month_num = monthly['month'].dt.year * 12 + monthly['month'].dt.month\n",
    "monthly['account_tenure_months'] = (month_num - start_month_num).fillna(0).astype(int)\n",
    "monthly.loc[monthly['account_tenure_months'] < 0, 'account_tenure_months'] = 0\n",
    "\n",
    "# Housekeeping: collect static feature column names\n",
    "static_feature_cols = ['age_at_t','age_bucket','income','income_bucket','region','account_tenure_months']\n",
    "core_transforms['static_feature_cols'] = static_feature_cols\n",
    "\n",
    "print('Added static user features:', static_feature_cols)\n",
    "monthly[['user_id','month'] + static_feature_cols].head(12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uw (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
